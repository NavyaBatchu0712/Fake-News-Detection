Fake News Detection using Various Attention Mechanisms and BERT
This project focuses on the detection of fake news leveraging state-of-the-art attention mechanisms and BERT (Bidirectional Encoder Representations from Transformers).
The goal is to develop robust models capable of accurately identifying misinformation in textual data.

Overview:

The project explores the following attention mechanisms in conjunction with BERT for fake news detection:

Self Attention
Self Attention + VGG16
Co-Attention
Co-Attention + ResNet
Cross Attention
Cross Attention + ResNet50
BERT + Self Attention

Methodology:

Each attention mechanism is implemented and evaluated for its effectiveness in discerning between real and fake news.
Models are trained and tested on benchmark datasets, with performance metrics such as accuracy, precision, recall, and F1 score used for evaluation.

Implementation Details:

Programming Language: Python
Libraries Used: Transformers, TensorFlow, Keras, Hugging Face Transformers
Pre-trained Models: BERT , VGG16, ResNet, ResNet50
Evaluation Metrics: Accuracy, Precision, Recall, F1 Score
Deployment: Models can be deployed using various platforms such as Flask or Docker for real-world applications.
